{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmfedOWt2eWd"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5PWCR1ia0eL"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iM63liNW49UA",
        "outputId": "7665674a-2b3a-4fe4-b799-0fbcd32e2ba8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting diptest\n",
            "  Downloading diptest-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.7/120.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.10/dist-packages (from diptest) (1.22.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from diptest) (5.9.5)\n",
            "Installing collected packages: diptest\n",
            "Successfully installed diptest-0.5.2\n"
          ]
        }
      ],
      "source": [
        "pip install diptest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBCxTSnu2cRr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.metrics import normalized_mutual_info_score as nmi\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import diptest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuyTClvJIw-3"
      },
      "source": [
        "# Math functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEdyj5_jFyCg"
      },
      "outputs": [],
      "source": [
        "def squared_euclidean_distance(centers, embedded_data):\n",
        "    tensor1 = centers.unsqueeze(0) #add one dim in front\n",
        "    tensor2 = embedded_data.unsqueeze(1) #add one dim in last (same as .unsqueeze(-1))\n",
        "    sed = (tensor1 - tensor2).pow(2).sum(2)  #power by 2 -> sum along the axis 2\n",
        "    return sed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyLJsMgxOwhP"
      },
      "outputs": [],
      "source": [
        "def z_score(data):\n",
        "    return (data - np.mean(data)) / np.std(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMcwa4y7haV8"
      },
      "outputs": [],
      "source": [
        "def int_to_one_hot(label_tensor, n_labels):\n",
        "    onehot = torch.zeros([label_tensor.shape[0], n_labels], dtype=torch.float, device=label_tensor.device)\n",
        "    onehot.scatter_(1, label_tensor.unsqueeze(1).long(), 1.0)\n",
        "    return onehot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fMTQDw76vi_"
      },
      "source": [
        "# Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRoBSbHH73Vs"
      },
      "source": [
        "Image datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kc8vkHjx4_mM"
      },
      "outputs": [],
      "source": [
        "def data_preprocess(train, test):\n",
        "  data = torch.cat([train.data, test.data], dim=0) #torch.Size([60000+10000, 28, 28]) for MNIST\n",
        "  labels = torch.cat([train.targets, test.targets], dim=0)\n",
        "\n",
        "  if data.dim() == 3:\n",
        "    data = data.reshape(-1, data.shape[1] * data.shape[2]) #flatten 28 x 28 images into 784 array (one dim vector) for MNIST\n",
        "  else:\n",
        "    data = data.reshape(-1, data.shape[1] * data.shape[2] * data.shape[3])\n",
        "\n",
        "  #move data to CPU, convert to numpy, and set requires_grad = False\n",
        "  data = data.detach().cpu().numpy()\n",
        "  labels = labels.detach().cpu().numpy()\n",
        "\n",
        "  data = z_score(data) #Z-score normalization\n",
        "\n",
        "  return data, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fP-pxzCMljFa"
      },
      "outputs": [],
      "source": [
        "def MNIST():\n",
        "  trainset = datasets.MNIST(root='./data', train=True, download=True)\n",
        "  testset = datasets.MNIST(root='./data', train=False, download=True)\n",
        "\n",
        "  data, labels = data_preprocess(trainset, testset)\n",
        "\n",
        "  return data, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUeNSiwg4M2_"
      },
      "outputs": [],
      "source": [
        "def FashionMNIST():\n",
        "  trainset = datasets.FashionMNIST(root='./data', train=True, download=True)\n",
        "  testset = datasets.FashionMNIST(root='./data', train=False, download=True)\n",
        "  \n",
        "  data, labels = data_preprocess(trainset, testset)\n",
        "\n",
        "  return data, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9Ev1LYs4zJa"
      },
      "outputs": [],
      "source": [
        "def KMNIST():\n",
        "  trainset = datasets.KMNIST(root='./data', train=True, download=True)\n",
        "  testset = datasets.KMNIST(root='./data', train=False, download=True)\n",
        "  \n",
        "  data, labels = data_preprocess(trainset, testset)\n",
        "\n",
        "  return data, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHmmWzyO7lvI"
      },
      "outputs": [],
      "source": [
        "def USPS():\n",
        "  trainset = datasets.USPS(root='./data', train=True, download=True)\n",
        "  testset = datasets.USPS(root='./data', train=False, download=True)\n",
        "  \n",
        "  data = np.r_[trainset.data, testset.data]\n",
        "  labels = np.r_[trainset.targets, testset.targets]\n",
        "  data = data.reshape(-1, 256)\n",
        "\n",
        "  data = z_score(data)\n",
        "\n",
        "  return data, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyoN4v2PrneB"
      },
      "outputs": [],
      "source": [
        "def load_data(filename):\n",
        "  dataset = np.genfromtxt(filename, delimiter=\",\")\n",
        "  return dataset[:, :-1], dataset[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IICE3KQNkk27"
      },
      "outputs": [],
      "source": [
        "def OPTDIGITS():\n",
        "  trainset = load_data('optdigits.tra')\n",
        "  testset = load_data('optdigits.tes')\n",
        "\n",
        "  data = np.r_[trainset[0], testset[0]]\n",
        "  labels = np.r_[trainset[1], testset[1]]\n",
        "\n",
        "  data = z_score(data)\n",
        "  \n",
        "  return data, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55Mwn5W1lEJe"
      },
      "outputs": [],
      "source": [
        "def PENDIGITS():\n",
        "  trainset = load_data('pendigits.tra')\n",
        "  testset = load_data('pendigits.tes')\n",
        "\n",
        "  data = np.r_[trainset[0], testset[0]]\n",
        "  labels = np.r_[trainset[1], testset[1]]\n",
        "\n",
        "  data = scale(data, axis=0)\n",
        "\n",
        "  return data, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDEsP92D4mx9"
      },
      "source": [
        "# Dip-test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol_am04UI052"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eB1wopN6Zs_"
      },
      "outputs": [],
      "source": [
        "# generate some bimodal random draws\n",
        "N = 1000\n",
        "hN = N // 2\n",
        "x = np.empty(N, dtype=np.float64)\n",
        "x[:hN] = np.random.normal(0.4, 1.0, hN)\n",
        "x[hN:] = np.random.normal(-0.4, 1.0, hN)\n",
        "\n",
        "# only the dip statistic\n",
        "dip = diptest.dipstat(x)\n",
        "\n",
        "# both the dip statistic and p-value\n",
        "dip, pval = diptest.diptest(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dzVWnud405i"
      },
      "source": [
        "# AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5B1M3Pz4CDg"
      },
      "outputs": [],
      "source": [
        "def torch_device():\n",
        "  return torch.device('cuda' if torch.cuda.is_available() else 'cpu') #return device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_d8sTSVvLRC"
      },
      "outputs": [],
      "source": [
        "def encode_batch(dataloader, autoenc, device): #embed the dataset in a mini-batch fashion\n",
        "    embeddings = [autoenc.encode(batch.to(device)).detach().cpu() for batch in dataloader]\n",
        "    return torch.cat(embeddings, dim=0).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BT3iG1fVkHD-"
      },
      "outputs": [],
      "source": [
        "def nearest_points_to_optimal_centers(X, optimal_centers, embedded_data):\n",
        "    best_center_points = np.argmin(cdist(optimal_centers, embedded_data), axis=1) #eq.2 \n",
        "    return X[best_center_points, :], embedded_data[best_center_points, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsL1yn-Xqk3A"
      },
      "outputs": [],
      "source": [
        "def nearest_points(large_cluster, center, small_cluster, max_diff_factor, min_size):\n",
        "    nearest_points = np.argsort(cdist(large_cluster, [center]), axis=0)\n",
        "    sample_size = small_cluster * max_diff_factor\n",
        "\n",
        "    if small_cluster + sample_size < min_size: # Check if more points should be taken because the other cluster is too small\n",
        "        sample_size = min(min_size - small_cluster, len(large_cluster))\n",
        "\n",
        "    return large_cluster[nearest_points[:sample_size, 0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pl9mPuAghRNx"
      },
      "outputs": [],
      "source": [
        "def iterate_over_batches(iteration, train_set, device, autoencoder, centers_torch, loss_fn, labels_torch, c_current, matrix_final, loss_weight, optimizer):\n",
        "  for batch, ids in train_set:  # Iterate over batches\n",
        "      batch_data = batch.to(device)\n",
        "      embedded = autoencoder.encode(batch_data)\n",
        "      reconstruction = autoencoder.decode(embedded)\n",
        "      embedded_centers_torch = autoencoder.encode(centers_torch)\n",
        "      ae_loss = loss_fn(reconstruction, batch_data) # Reconstruction Loss\n",
        "      squared_diffs = squared_euclidean_distance(embedded_centers_torch, embedded) # Get distances between points and centers. Get nearest center\n",
        "      current_labels = squared_diffs.argmin(1) if iteration != 0 else labels_torch[ids]\n",
        "      escaped_diffs = torch.matmul(int_to_one_hot(current_labels, c_current).float(), matrix_final) * squared_diffs\n",
        "      squared_center_diffs = squared_euclidean_distance(embedded_centers_torch, embedded_centers_torch) # Normalize loss by cluster distances\n",
        "      mask = torch.where(squared_center_diffs != 0) # Ignore zero values (diagonal)\n",
        "      sqrt_masked_center_diffs = squared_center_diffs[mask[0], mask[1]].sqrt()\n",
        "      masked_center_diffs_std = sqrt_masked_center_diffs.std() if len(sqrt_masked_center_diffs) > 2 else 0\n",
        "      cluster_loss = escaped_diffs.sum(1).mean() * (1 + masked_center_diffs_std) / sqrt_masked_center_diffs.mean()\n",
        "      loss = ae_loss + (cluster_loss * loss_weight)# Loss function\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()       # Backward pass\n",
        "      optimizer.step()\n",
        "  return ae_loss, cluster_loss, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNYhkuASsWQx"
      },
      "outputs": [],
      "source": [
        "def dip_deck_training(X, c_current, threshold, loss_weight, centers_cpu, labels_cpu, matrix_cpu, c_max, c_min, d_epochs, optimizer, loss_fn, autoencoder, device, train_set, test_set, diff_factor, debug):\n",
        "    i = 0\n",
        "    while i < d_epochs:\n",
        "        labels_torch = torch.from_numpy(labels_cpu).long().to(device)\n",
        "        centers_torch = torch.from_numpy(centers_cpu).float().to(device)\n",
        "        matrix_torch = torch.from_numpy(matrix_cpu).float().to(device)\n",
        "        dip_matrix_eye = matrix_torch + torch.eye(c_current, device=device)# Get dip costs matrix\n",
        "        \n",
        "        ae_loss, cluster_loss, loss = iterate_over_batches(i, train_set, device, autoencoder, centers_torch, loss_fn, labels_torch, c_current, dip_matrix_eye / dip_matrix_eye.sum(1).reshape((-1, 1)), loss_weight, optimizer)\n",
        "        \n",
        "        # labels_cpu = labels_torch.detach().cpu().numpy()\n",
        "        embedded_data = encode_batch(test_set, autoencoder, device)     # Update centers\n",
        "        embedded_centers_cpu = autoencoder.encode(centers_torch).detach().cpu().numpy()\n",
        "        labels_cpu = np.argmin(cdist(embedded_centers_cpu, embedded_data), axis=0)\n",
        "        optimal_centers = np.array([np.mean(embedded_data[labels_cpu == cluster_id], axis=0) for cluster_id in range(c_current)])\n",
        "        centers_cpu, embedded_centers_cpu = nearest_points_to_optimal_centers(X, optimal_centers, embedded_data)\n",
        "        matrix_cpu = dip_matrix(embedded_data, embedded_centers_cpu, labels_cpu, c_current, diff_factor) # Update Dips\n",
        "\n",
        "        if debug:\n",
        "          print(\"Iteration {0}  (n_clusters = {4}) - reconstruction loss: {1} / cluster loss: {2} / total loss: {3}\".format(i, ae_loss.item(), cluster_loss.item(), loss.item(), c_current) + \"\\nmax dip\", np.max(matrix_cpu), \" at \", np.unravel_index(np.argmax(matrix_cpu, axis=None), matrix_cpu.shape))\n",
        "        \n",
        "        i += 1 # i is increased here. Else next iteration will start with i = 1 instead of 0 after a merge\n",
        "        dip_argmax = np.unravel_index(np.argmax(matrix_cpu, axis=None), matrix_cpu.shape) # Start merging procedure\n",
        "        if i != 0: # Is merge possible?\n",
        "            while matrix_cpu[dip_argmax] >= threshold and c_current > c_min:\n",
        "                if debug:\n",
        "                  print(\"Start merging in iteration {0}.\\nMerging clusters {1} with dip value {2}.\".format(i, dip_argmax, matrix_cpu[dip_argmax]))\n",
        "                i = 0 # Reset iteration and reduce number of cluster\n",
        "                c_current -= 1\n",
        "                labels_cpu, centers_cpu, embedded_centers_cpu, matrix_cpu = merge_by_dip_value(X, embedded_data, labels_cpu, dip_argmax, c_current, centers_cpu, embedded_centers_cpu, diff_factor)\n",
        "                dip_argmax = np.unravel_index(np.argmax(matrix_cpu, axis=None), matrix_cpu.shape)\n",
        "        if c_current == 1:\n",
        "            if debug:\n",
        "              print(\"Only one cluster left\")\n",
        "            break\n",
        "    return labels_cpu, c_current, centers_cpu, autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvxAwbCfbmGV"
      },
      "outputs": [],
      "source": [
        "def check_clusters_sizes_diff(i, j, points_in_i, points_in_j, points_in_i_or_j, dip_centers, center_diff, diff_factor, dip_p_value, min_sample_size):\n",
        "  if points_in_i.shape[0] > points_in_j.shape[0] * diff_factor or points_in_j.shape[0] > points_in_i.shape[0] * diff_factor:\n",
        "      if points_in_i.shape[0] > points_in_j.shape[0] * diff_factor:\n",
        "          points_in_i = nearest_points(points_in_i, dip_centers[j], points_in_j.shape[0], diff_factor, min_sample_size)\n",
        "      elif points_in_j.shape[0] > points_in_i.shape[0] * diff_factor:\n",
        "          points_in_j = nearest_points(points_in_j, dip_centers[i], points_in_i.shape[0], diff_factor, min_sample_size)\n",
        "      \n",
        "      points_in_i_or_j = np.append(points_in_i, points_in_j, axis=0)\n",
        "      proj_points = np.dot(points_in_i_or_j, center_diff)\n",
        "      _, dip_p_value_2 = diptest.diptest(proj_points)\n",
        "      dip_p_value = min(dip_p_value, dip_p_value_2)\n",
        "  return dip_p_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gUBvzyhqaHr"
      },
      "outputs": [],
      "source": [
        "def dip_matrix(data, dip_centers, dip_labels, n_clusters, diff_factor=2, min_sample_size=100):\n",
        "    dip_matrix = np.zeros((n_clusters, n_clusters))    # Loop over all combinations of centers\n",
        "    for i in range(0, n_clusters - 1):\n",
        "        for j in range(i + 1, n_clusters):\n",
        "            center_diff = dip_centers[i] - dip_centers[j]\n",
        "            \n",
        "            points_in_i = data[dip_labels == i]\n",
        "            points_in_j = data[dip_labels == j]\n",
        "            points_in_i_or_j = np.append(points_in_i, points_in_j, axis=0)\n",
        "            \n",
        "            proj_points = np.dot(points_in_i_or_j, center_diff)\n",
        "            _, dip_p_value = diptest.diptest(proj_points)\n",
        "            # Check if clusters sizes differ heavily\n",
        "            dip_p_value = check_clusters_sizes_diff(i, j, points_in_i, points_in_j, points_in_i_or_j, dip_centers, center_diff, diff_factor, dip_p_value, min_sample_size)\n",
        "            # Add pval to dip matrix\n",
        "            dip_matrix[i][j] = dip_p_value\n",
        "            dip_matrix[j][i] = dip_p_value\n",
        "    return dip_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkr7nsz6xbhR"
      },
      "outputs": [],
      "source": [
        "def update_labels(labels_cpu, dip_argmax, c_current):\n",
        "  for j, l in enumerate(labels_cpu):\n",
        "    if l == dip_argmax[0] or l == dip_argmax[1]:\n",
        "        labels_cpu[j] = c_current - 1\n",
        "    elif l < dip_argmax[0] and l < dip_argmax[1]:\n",
        "        labels_cpu[j] = l\n",
        "    elif l > dip_argmax[0] and l > dip_argmax[1]:\n",
        "        labels_cpu[j] = l - 2\n",
        "    else:\n",
        "        labels_cpu[j] = l - 1\n",
        "  return labels_cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzAlhoI9q-y5"
      },
      "outputs": [],
      "source": [
        "def merge_by_dip_value(X, embedded_data, labels_cpu, dip_argmax, c_current, centers_cpu, embedded_centers_cpu, diff_factor):\n",
        "    points_in_center_1 = len(labels_cpu[labels_cpu == dip_argmax[0]])     # Get points in clusters\n",
        "    points_in_center_2 = len(labels_cpu[labels_cpu == dip_argmax[1]])\n",
        "    labels_cpu = update_labels(labels_cpu, dip_argmax, c_current)# update labels\n",
        "    # Find new center position\n",
        "    optimal_new_center = (embedded_centers_cpu[dip_argmax[0]] * points_in_center_1 + embedded_centers_cpu[dip_argmax[1]] * points_in_center_2) / (points_in_center_1 + points_in_center_2) \n",
        "    new_center_cpu, new_embedded_center_cpu = nearest_points_to_optimal_centers(X, [optimal_new_center], embedded_data)\n",
        "    centers_cpu = np.append(np.delete(centers_cpu, dip_argmax, axis=0), new_center_cpu, axis=0)  # Remove the two old centers and add the new one\n",
        "    embedded_centers_cpu = np.append(np.delete(embedded_centers_cpu, dip_argmax, axis=0), new_embedded_center_cpu, axis=0)\n",
        "    dip_matrix_cpu = dip_matrix(embedded_data, embedded_centers_cpu, labels_cpu, c_current, diff_factor) # Update dip values\n",
        "    return labels_cpu, centers_cpu, embedded_centers_cpu, dip_matrix_cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybP6k_P8vjoC"
      },
      "outputs": [],
      "source": [
        "def train_autoencoder(trainloader, learning_rate, n_epochs, device, optimizer, loss_fn, input_dim, embedding_size, autoencoder_class):\n",
        "    if embedding_size > input_dim:\n",
        "      embedding_size = input_dim\n",
        "    autoencoder = autoencoder_class(input_dim=input_dim, embedding_size=embedding_size).to(device) #pretrain Autoencoder\n",
        "    optimizer = optimizer(autoencoder.parameters(), lr=learning_rate)\n",
        "    autoencoder.start_training(trainloader, n_epochs, device, optimizer, loss_fn)\n",
        "    return autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMcAv3NBsYSo"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(torch.nn.Module):\n",
        "   def __init__(self, input_dim, embedding_size):\n",
        "      super(Autoencoder, self).__init__()\n",
        "\n",
        "      self.encoder = torch.nn.Sequential(\n",
        "         torch.nn.Linear(input_dim, 500), #input_dim = d\n",
        "         torch.nn.LeakyReLU(inplace=True),\n",
        "         torch.nn.Linear(500, 500),\n",
        "         torch.nn.LeakyReLU(inplace=True),\n",
        "         torch.nn.Linear(500, 2000),\n",
        "         torch.nn.LeakyReLU(inplace=True),\n",
        "         torch.nn.Linear(2000, embedding_size), #embedding = m\n",
        "      )\n",
        "      #reverse of encoder\n",
        "      self.decoder = torch.nn.Sequential(\n",
        "         torch.nn.Linear(embedding_size, 2000),\n",
        "         torch.nn.LeakyReLU(inplace=True),\n",
        "         torch.nn.Linear(2000, 500),\n",
        "         torch.nn.LeakyReLU(inplace=True),\n",
        "         torch.nn.Linear(500, 500),\n",
        "         torch.nn.LeakyReLU(inplace=True),\n",
        "         torch.nn.Linear(500, input_dim),\n",
        "      )\n",
        "\n",
        "   def encode(self, x):\n",
        "     return self.encoder(x)\n",
        "\n",
        "   def decode(self, encoded):\n",
        "     return self.decoder(encoded)\n",
        "\n",
        "   def forward(self, x):\n",
        "      encoded = self.encoder(x)\n",
        "      decoded = self.decoder(encoded)\n",
        "      return decoded\n",
        "\n",
        "   def start_training(self, trainloader, n_epochs, device, optimizer, loss_fn):\n",
        "      for _ in range(n_epochs):\n",
        "        for batch, _ in trainloader:\n",
        "          batch_data = batch.to(device)\n",
        "          decoded = self.forward(batch_data)\n",
        "          # Compute the loss \n",
        "          loss = loss_fn(decoded, batch_data)\n",
        "          # Zero your gradients for every batch\n",
        "          optimizer.zero_grad()\n",
        "          # Compute the gradients of the loss\n",
        "          loss.backward()\n",
        "          # Adjust learning weights\n",
        "          optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uZCyKxy43G2"
      },
      "source": [
        "# DipDeck function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjGe4X_aBuhL"
      },
      "outputs": [],
      "source": [
        "def handeling_errors(c_start, c_max, c_min):\n",
        "    if c_max < c_min:\n",
        "        raise Exception(\"n_clusters_max can not be smaller than n_clusters_min\")\n",
        "    if c_min <= 0:\n",
        "        raise Exception(\"n_clusters_min must be greater than zero\")\n",
        "    if c_start < c_min:\n",
        "        raise Exception(\"n_clusters can not be smaller than n_clusters_min\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqFuqGjWW6TQ"
      },
      "outputs": [],
      "source": [
        "def data_loader(X, batch_size):\n",
        "  train_set = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(*(torch.from_numpy(X).float(), torch.arange(0, X.shape[0]))),\n",
        "                                          batch_size=batch_size, # sample random mini-batches from the data\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=False) # create a Dataloader to test the autoencoder in mini-batch fashion (Important for validation)\n",
        "  test_set = torch.utils.data.DataLoader(torch.from_numpy(X).float(),\n",
        "                                          batch_size= batch_size,\n",
        "                                          shuffle=False, # Note that we deactivate the shuffling\n",
        "                                          drop_last=False)\n",
        "  return train_set, test_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTm9n0yXrzJO"
      },
      "outputs": [],
      "source": [
        "def dip_deck(X, c_start, threshold, loss_weight, c_max, c_min, batch_size, learning_rate, pt_epochs, d_epochs, optimizer, loss_fn, autoencoder, embedding_size, max_diff_factor, debug):\n",
        "    handeling_errors(c_start, c_max, c_min)\n",
        "    device = torch_device()\n",
        "    trainloader, testloader = data_loader(X, batch_size)\n",
        "\n",
        "    if autoencoder is None:\n",
        "        autoencoder = train_autoencoder(trainloader, learning_rate, pt_epochs, device, optimizer, loss_fn, X.shape[1], embedding_size, Autoencoder)\n",
        "    \n",
        "    # Execute kmeans in embedded space - initial clustering\n",
        "    embedded_data = encode_batch(testloader, autoencoder, device)\n",
        "    kmeans = KMeans(n_clusters= c_start)\n",
        "    kmeans.fit(embedded_data)\n",
        "\n",
        "    centers_cpu, embedded_centers_cpu = nearest_points_to_optimal_centers(X, kmeans.cluster_centers_, embedded_data)  # Get nearest points to optimal centers\n",
        "\n",
        "    c_labels_cpu, c_current, centers_cpu, autoencoder = dip_deck_training(X, c_start,     # Start training\n",
        "                                                                          threshold,\n",
        "                                                                          loss_weight,\n",
        "                                                                          centers_cpu,\n",
        "                                                                          kmeans.labels_,\n",
        "                                                                          dip_matrix(embedded_data, embedded_centers_cpu, kmeans.labels_, c_start, max_diff_factor), # Initial dip values\n",
        "                                                                          c_max, c_min, d_epochs,\n",
        "                                                                          optimizer(autoencoder.parameters(), lr= learning_rate * 0.1),\n",
        "                                                                          loss_fn, autoencoder, device, \n",
        "                                                                          trainloader, testloader,\n",
        "                                                                          max_diff_factor, debug)\n",
        "    return c_labels_cpu, c_current, centers_cpu, autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKg8f_T29OOm"
      },
      "outputs": [],
      "source": [
        "class DipDeck():\n",
        "\n",
        "  def __init__(self, n_clusters_start=35, dip_merge_threshold=0.9, cluster_loss_weight=1, n_clusters_max=np.inf, n_clusters_min=1, batch_size=256, learning_rate=1e-3, \n",
        "               pretrain_epochs=100, dedc_epochs=50, optimizer_class=torch.optim.Adam, loss_fn=torch.nn.MSELoss(), autoencoder=None, embedding_size=5, max_cluster_size_diff_factor=2, debug=False):\n",
        "        \n",
        "        self.n_clusters_start = n_clusters_start\n",
        "        self.dip_merge_threshold = dip_merge_threshold\n",
        "        self.cluster_loss_weight = cluster_loss_weight\n",
        "        self.n_clusters_max = n_clusters_max\n",
        "        self.n_clusters_min = n_clusters_min\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.pretrain_epochs = pretrain_epochs\n",
        "        self.dedc_epochs = dedc_epochs\n",
        "        self.optimizer_class = optimizer_class\n",
        "        self.loss_fn = loss_fn\n",
        "        self.autoencoder = autoencoder\n",
        "        self.embedding_size = embedding_size\n",
        "        self.max_cluster_size_diff_factor = max_cluster_size_diff_factor\n",
        "        self.debug = debug\n",
        "    \n",
        "\n",
        "  def fit(self, X):\n",
        "        labels, n_clusters, centers, autoencoder = dip_deck(X, self.n_clusters_start, self.dip_merge_threshold,\n",
        "                                                             self.cluster_loss_weight, self.n_clusters_max,\n",
        "                                                             self.n_clusters_min, self.batch_size, self.learning_rate,\n",
        "                                                             self.pretrain_epochs, self.dedc_epochs,\n",
        "                                                             self.optimizer_class,\n",
        "                                                             self.loss_fn, self.autoencoder, self.embedding_size,\n",
        "                                                             self.max_cluster_size_diff_factor, self.debug)\n",
        "        self.labels_ = labels\n",
        "        self.n_clusters_ = n_clusters\n",
        "        self.cluster_centers_ = centers\n",
        "        self.autoencoder = autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgW6nzsTatbx"
      },
      "source": [
        "# DipDeck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoOvahLdaw-R",
        "outputId": "907f9a84-7ba7-46f0-9838-21032a36abcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/usps.bz2 to ./data/usps.bz2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6579383/6579383 [00:01<00:00, 4227208.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/usps.t.bz2 to ./data/usps.t.bz2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1831726/1831726 [00:01<00:00, 1366286.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "K: 10\n",
            "NMI: 0.8646958326774905\n"
          ]
        }
      ],
      "source": [
        "#Datasets\n",
        "#data, labels = MNIST()\n",
        "#data, labels = FashionMNIST()\n",
        "#data, labels = KMNIST()\n",
        "data, labels = USPS()\n",
        "#data, labels = OPTDIGITS()\n",
        "#data, labels = PENDIGITS()\n",
        "\n",
        "\n",
        "model = DipDeck()\n",
        "model.fit(data)\n",
        "\n",
        "print(\"K:\", model.n_clusters_)\n",
        "print(\"NMI:\", nmi(labels, model.labels_))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "rmfedOWt2eWd",
        "LuyTClvJIw-3",
        "9fMTQDw76vi_",
        "GDEsP92D4mx9"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}